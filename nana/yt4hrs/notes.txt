https://www.youtube.com/watch?v=X48VuDVv0do

- what is kubernetes
- k8s architecture
- main k8s components
- minikube and kubectl
- main kubectl commands
- k8s yaml config files
- hands-on (deploy an app)

- organizing components using namespaces
- ingress
- helm
- volumes
- statefulsets
- service types (ClusterIP, NodePort, Headless, LoadBalancer)

===============================================================================

container orchestration tool

what problem it solves?
what are the tasks?

HA
Scalability
Disaster Recovery

===============================================================================

components

- node
- pod: 
    abstraction over a container
    smallest unit in k8s
    usually 1 app per pod
    how pods communicate? k8s gives them a virtual IP (ephemeral) -> service
- service:
    permamnent IP address (and LoadBalancer)
    the lifecycle of pods and servuces are not connected
    external service for app accessible from outside - internal service e.g. for db 
- ingress:
    to access from outside like: https://my-app.domain.com and not https://node-ip:port
- configmap:
    external configuration of your application
- secret:
    use secret and configmap as environment variables
- volumes
- deployment:
    a blueprint for defining pods
- stateful set:
    a blueprint for defining db (or similar)

===============================================================================

kubernetes architecture

worker nodes:
 3 essential components (processes): 
    container runtime (docker)
    kubelet: 
        interact with both container and node
        kubelet starts the pod and assigning resources
    kube proxy:
        forwards the requests (for example it routes a db request of an app to the db pod on the same node - reduce network overhead)

how to interact with the cluster? schedule pos, monitor, re-schedule/restart a pod, join a new node -> master node
master nodes:
4 essential components (processes):
    api server: 
        like the cluster gateway
        acts as a gatekeeper (authentication)
    scheduler:
        it is only decides where to put pods, the actual work is done by kubelet
    controller manager:
        detects cluster state changes (pods die)
        makes a request to the scheduler
    etcd:
        key-value store of the cluster state

===============================================================================

example cluster setup

===============================================================================

minikube and kubectl

===============================================================================

main kubectl commands

create and debug pods

CRUD commands (e.g create/edit/delete deployment)

status of different k8s components
k get nodes | pod | services | replicaset | deployment

debugging pods
k logs <pod-name>
k exec -it <pod-name> -- bin/bash

k create deployment nginx-depl --image=nginx:1.27.2-alpine-slim

k get deployments.apps
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
nginx-depl   0/1     1            0           10s

k get pods
NAME                         READY   STATUS              RESTARTS   AGE
nginx-depl-88bbbdf56-427j8   0/1     ContainerCreating   0          24s

pod name: <deployment-name>-<replicaset-id>-<id>

k get replicasets.apps
NAME                   DESIRED   CURRENT   READY   AGE
nginx-depl-88bbbdf56   1         1         1       78s

abstraction layers:
deployment
replicaset
pod
container

k edit deployments.apps nginx-depl

debugging pods
k logs nginx-depl-88bbbdf56-427j8
k describe pod nginx-depl-88bbbdf56-427j8 
k exec -it pods/nginx-depl-88bbbdf56-427j8 -- sh

delete deployment
k delete deployments.apps nginx-depl

k apply -f nginx-deployment.yaml

===============================================================================

k8s configuration yaml files

3 parts of a configuration file:
    metadata
    specification: different for each kind
    status: automatic by k8s (desired vs actual)

format of configuration files
strict indentation
use yaml validators

connecting components (labels, selectors, and ports)
in metadata we add labels (a key-value pair)
in deployments selectors will match labels of pods metadata, 
in services selectors will match labels of deployments metadata
service targetPort will connect to pods containerPort

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.16
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080

to verify that service is pointing to the correct pods
k describe service nginx-service
shows selector, targetPort, endpoint (ip:port, ip:port, ...)

to see IPs of pods:
k get pod -o wide

k get deployments.apps nginx-depl -o yaml > nginx-deployment.result.yaml

===============================================================================

application setup

db pod - internal service: db url (configmap), db username, db password (secret)
app pod - external service (accessible with IP:port for now) - deployment yaml: reference to above configmap and secret, environment variables

- create a pg deployment
- create a app deployment
- create a secret
- create a configmap
- apply secret and configmap: k apply -f infra/app-secret.yaml && k apply -f infra/app-configmap.yaml
- apply pvc: k apply -f infra/postgres-pvc.yaml
- apply pg deployment(and service): k apply -f infra/postgres-deployment.yaml
- create app image: docker buildx build . -t mini-go-pg-app:v0.1
- create app service: difference: type: LoadBalancer and nodePort: 30001
- apply app deployment(and service): k apply -f infra/app-deployment.yaml
    for getting an external-IP in minikube: minikube service <service-name>
    in rancher desktop:
      Option1: use NodePort type for now
      Option 2: use kubectl port-forward
      Option 3: use MetalLB for LoadBalancer Support
      * see ./misc/lb-rancher-desktop.md


NAME                              READY   STATUS    RESTARTS   AGE
pod/golang-app-5fbd74df79-gqpj4   1/1     Running   0          49s
pod/postgres-696fb5d4b6-c2rgh     1/1     Running   0          31m

NAME                         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
service/golang-app-service   LoadBalancer   10.43.221.14   <pending>     80:30001/TCP   49s
service/kubernetes           ClusterIP      10.43.0.1      <none>        443/TCP        8h
service/postgres-service     ClusterIP      10.43.73.149   <none>        5432/TCP       44m

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/golang-app   1/1     1            1           49s
deployment.apps/postgres     1/1     1            1           31m

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/golang-app-5fbd74df79   1         1         1       49s
replicaset.apps/postgres-696fb5d4b6     1         1         1       31m

curl http://localhost:30001/books ðŸŽ‰

===============================================================================

k8s namespaces

like a virtual cluster inside the cluster

k get namespaces
kube-system: only for the system itself
kune-public: it has a configmap
kube-node-lease: keep info about heartbeat of the node
default: for us to create resources if we don't specify any namespace

create namespace using config file

usecases:
groupping resources: i.e. database, monitoring, elasticsearch, nginx-ingress, ...
having multiple teams: similar deployments may overwrite the other
resource sharing in staging and development: 
  i.e reuse elasticsearch in both development and staging
  blue/green in production (different versions of production app)
access and resource limits: again in having multiple teams
  set resource quota to prevent a team to starve others
  secure resources inside a team

you cannot access most resources from another namespace: service can be shared (i.e. db-service)

some components cannot get isolated inside namespaces (they are global): i.e. node, volumes
  k api-resources --namespaced=false

1. k apply -f <config.yaml> --namespace=default
2. kind: ...
   metadata:
      name: ...
      namespace: ...

change active namespace with kubens/kubectx tool

===============================================================================

kubernetes ingress

external service vs ingress: http://ip:port -> http://domain.com
app-pod <- app-service <- app-ingress <- outside request

config file:
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: myapp-ingress
  spec:
    rules: # routing rules
    - host: domain.com # we should map node's IP (entrypoint server) to a valid domain
      http: # protocol (https later) - it's not the same as request in browser scheme (outside request)
        paths: # url-path: anything comes after domain.com/...
        - backend: # target that incoming requests will be redirected (kind: Service)
            serviceName: myapp-internal-service # metadata: name of app service
            servicePort: 8080

in addition to ingress config file we need an implementation for ingress which is: Ingress Controller
which is another pod (or set of pods) that run indside the cluster 
it evaluates and process the routing rules (managing redirects)
it is the entrypoint of the cluster

list:
from k8s itself: k8s Nginx Ingress Controller
...

entrypoint:
  cloud providers: specific loadbalancer
  bare metal: external proxy server (hardware or software), 


in minikube: install ingress controller:
  minikube addons enable ingress
  you can see it using: k get pod -n kube-system

other routing rules:
- multiple paths for a same host: separate backends using multiple paths
- multiple sub-domains or domains: separate backends using multiple hosts

configuring TLS certificate: 
  spec: tls: host: ... secretName: myapp-secret-tls (points to a secret: ...metadata: name: myapp-secret-tls ...)

===============================================================================

helm

package manager for kubernetes
to package/bundle yaml files (helm charts)

helm search <keyword>
https://artifacthub.io/
helm chart private repository

templating engine: for similar microservices or one app in several environment (dev/stage/prod) 
{{ .Values.name }} -> values.yaml or --set flag

practical in CI/CD

mychart/
  Chart.yaml
  values.yaml
  charts/
  templates/
  ...

valuse injection into template files

helm install --values=my-values.yaml <chartname>

values.yaml         my-values.yaml
imageName: myapp    version: 2.0.0 # just version is overwritten
port: 8080
version: 1.0.0

release management
in helm v2 there was a server to manageing and keeping history of installed versions called Tiller
in helm v3 it got removed

===============================================================================

kubernetes volumes

1. persistent volume
2. persistent volume claim
3. storage class

usecases:
storage for statefuls like db
predefined directory for an app to use as filesystem

persistent volume is a resource just like ram and cpu
created from yaml file: kind: PersistentVolume
is gets actual physical storage from: nodes local disk, nfs server, cloud storage, ...

kubernetes just provide an interface 
we should decide what type of storage we use
and create and manage them

persistent volumes are not namespaced

local persistent volumes violate two requirements for date persistence:
1. not being tied to one specific node
2. not surviving cluster crashes

for db persistence use remote storage

as persistent volumes are resources and should be available
when using them in apps, providing them before hand is the 
job of amin.

the job of application developer is to claim the persistent volume using pvc

pv <- pvc <- app-pod

configmap and secret are another local storage type which managed by kubernetes itself

for admins adding manually lots of pv could get tedious.
storage class provisions persistent volumes dynamically whenever pvc claims it

kind: StorageClass
...
provisioner: ... # (internal and external provisioner)
pv <--dynamically-- sc <- pvc <- app-pod

===============================================================================

statefulsets

stateless -> deployment -> replica & pv 

stateful -> statefulset -> replica & pv

stateful apps are created from same specification but not interchanable
statefulsets creates a sticky identity for each replica
if a pod dies, the new one keeps its identity

for example for db, only one pod is for read and write (master)
the pods don't use the same physical storage (data is the same but not in same path. they are sync)

there is service for statefulsets just like deployments

tasks that we should do ourselves:
configuring the cloning and synchronization
make remote storage available
managing and backup

===============================================================================

k8s services

ClusterIP
Headless
NodePort
LoadBalancer

multi-port service: when srevice exposes multiple ports i.e. postgres and postgres-expoerter for prometheus
in that case ports: should have - name: ... attribute for each exposed port

Headless: when client wants to talk to specific replica (pod) directly without going through service
it is needed in stateful applications. (remember pods in stateful apps are not identical - and master only allowed to write)
in this case client should lookup pod IP (DNS lookup)

DNS lookup for service: returns single IP address (ClusterIP)
set ClusterIP to `None` (in kind: Service - Headless Service): returns pod IP address instead 

NodePort: creates a port which is accessible through external trafic (it is available in every worker nodes)
30000 - 32767
not efficient and not secure

better alternative is LoadBalancer
when define LoadBalancer, NodePort and ClusterIP service are created automatically by kubernetes

LoadBalancer is an extension of NodePort which is an extension of ClusterIP

NodePort is not for production
in production either LoadBalancer of cloud providers or ingress

===============================================================================
